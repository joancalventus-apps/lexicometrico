import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
import networkx as nx
from textblob import TextBlob
from wordcloud import WordCloud
from collections import Counter
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.cluster import KMeans
from scipy.stats import chi2_contingency, norm
import spacy

# --- 1. CONFIGURACI√ìN DE P√ÅGINA ---
st.set_page_config(page_title="Lexicom√©trico", layout="wide")

# --- INYECCI√ìN CSS ---
st.markdown("""
    <style>
    .block-container {padding-top: 1rem; padding-bottom: 5rem;}
    
    /* ETIQUETAS PESTA√ëAS (Ajustado: un puntito menos) */
    .stTabs [data-baseweb="tab-list"] button [data-testid="stMarkdownContainer"] p {
        font-size: 1.1rem; /* Reducido de 1.3rem a 1.1rem */
        font-weight: 600;
    }
    
    .stDataFrame {font-size: 1.0rem;}
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    </style>
""", unsafe_allow_html=True)

# --- 2. GESTI√ìN DE ESTADO ---
if 'selected_word' not in st.session_state:
    st.session_state['selected_word'] = None

# --- 3. CONFIGURACI√ìN GLOBAL ---
LANG_MAP = {'Espa√±ol': 'spanish', 'Ingl√©s': 'english'}

@st.cache_resource
def download_nltk_resources():
    resources = ['punkt', 'punkt_tab', 'stopwords']
    for res in resources:
        try:
            nltk.data.find(f'tokenizers/{res}')
        except LookupError:
            nltk.download(res)
        except ValueError:
            try: nltk.data.find(f'corpora/{res}')
            except LookupError: nltk.download(res)
download_nltk_resources()

@st.cache_resource
def load_spacy_model():
    model_name = "es_core_news_sm"
    try: nlp = spacy.load(model_name)
    except OSError:
        from spacy.cli import download
        download(model_name)
        nlp = spacy.load(model_name)
    return nlp

# --- 4. FUNCIONES ---
def clean_text(text, language='Espa√±ol', custom_stops=[], min_len=2, apply_lemma=False):
    if pd.isna(text): return []
    lang_code = LANG_MAP.get(language, 'spanish')
    stop_words = set(stopwords.words(lang_code))
    stop_words.update(set(custom_stops))
    
    if apply_lemma and language == 'Espa√±ol':
        nlp = load_spacy_model()
        nlp.max_length = 2000000 
        doc = nlp(str(text).lower())
        tokens = [token.lemma_ for token in doc if token.is_alpha and token.lemma_ not in stop_words and len(token.lemma_) >= min_len]
    else:
        tokens = word_tokenize(str(text).lower())
        tokens = [word for word in tokens if word.isalpha() and word not in stop_words and len(word) >= min_len]
    return tokens

def get_significance_stars(p_value):
    if p_value < 0.001: return "***"
    if p_value < 0.01:  return "**"
    if p_value < 0.05:  return "*"
    return "NS"

def calculate_mtld(tokens, threshold=0.72):
    def count_factors(token_list):
        factors = 0; ttr = 1.0; word_set = set(); length = 0
        for word in token_list:
            length += 1; word_set.add(word); ttr = len(word_set) / length
            if ttr < threshold: factors += 1; length = 0; word_set = set(); ttr = 1.0
        if length > 0: factors += (1 - ttr) / (1 - threshold)
        return factors
    if not tokens: return 0
    return len(tokens) / ((count_factors(tokens) + count_factors(tokens[::-1])) / 2)

# Funci√≥n matem√°tica para An√°lisis de Correspondencia Simple (SVD)
def simple_correspondence_analysis(contingency_table):
    # Matriz de frecuencias
    X = contingency_table.values
    rows, cols = X.shape
    
    # Masa total
    N = np.sum(X)
    
    # Matriz de correspondencia
    P = X / N
    
    # Masas de filas y columnas
    r = np.sum(P, axis=1)
    c = np.sum(P, axis=0)
    
    # Diagonales inversas
    Dr_inv_sqrt = np.diag(1 / np.sqrt(r))
    Dc_inv_sqrt = np.diag(1 / np.sqrt(c))
    
    # Matriz de residuos estandarizados
    # S = (P - r * c') / sqrt(r * c')
    # Equivalent to calculating SVD on deviation matrix
    
    # C√°lculo manual optimizado para SVD
    expected = np.outer(r, c)
    Z = (P - expected) / np.sqrt(expected)
    
    # SVD
    U, s, Vt = np.linalg.svd(Z, full_matrices=False)
    
    # Coordenadas principales (2 dimensiones)
    # Filas (Categor√≠as)
    row_coords = Dr_inv_sqrt @ U[:, :2] @ np.diag(s[:2])
    # Columnas (Palabras)
    col_coords = Dc_inv_sqrt @ Vt.T[:, :2] @ np.diag(s[:2])
    
    return row_coords, col_coords, s

# --- 5. INTERFAZ ---
st.title("üìä Lexicom√©trico")

st.sidebar.header("1. Carga de Datos")
uploaded_file = st.sidebar.file_uploader("Arrastrar y soltar archivo CSV aqu√≠", type=["csv"])
lang_opt = st.sidebar.selectbox("Idioma del texto", ["Espa√±ol", "Ingl√©s"])

st.sidebar.markdown("---")
st.sidebar.header("2. Procesamiento")
use_lemmatization = st.sidebar.checkbox("Aplicar Lematizaci√≥n", help="Agrupa variantes (ej: pol√≠ticos, pol√≠tica -> pol√≠tico)")

st.sidebar.header("3. Filtros")
min_freq_filter = st.sidebar.slider("Seleccione Frecuencia m√≠nima de aparici√≥n:", 1, 50, 2)
custom_stopwords_input = st.sidebar.text_area("Excluir palabras (separar por coma):", placeholder="ej: respuesta, ns, nc")
custom_stopwords_list = [x.strip().lower() for x in custom_stopwords_input.split(',')] if custom_stopwords_input else []

if uploaded_file is not None:
    try:
        df = pd.read_csv(uploaded_file)
        text_col = df.columns[-1]
        cat_cols = df.columns[:-1].tolist()

        with st.spinner('Procesando corpus textual...'):
            df['tokens'] = df[text_col].apply(lambda x: clean_text(x, lang_opt, custom_stopwords_list, 2, use_lemmatization))
            all_tokens_raw = [t for sub in df['tokens'] for t in sub]
            freq_raw = Counter(all_tokens_raw)
            valid_words = set(w for w, c in freq_raw.items() if c >= min_freq_filter)
            df['tokens'] = df['tokens'].apply(lambda tokens: [t for t in tokens if t in valid_words])
            df['str_processed'] = df['tokens'].apply(lambda x: ' '.join(x))
            df['polaridad'] = df[text_col].apply(lambda x: TextBlob(str(x)).sentiment.polarity)
            all_tokens = [token for sublist in df['tokens'] for token in sublist]

        if len(all_tokens) == 0:
            st.error("No hay palabras suficientes con los filtros actuales.")
        else:
            freq_dist = Counter(all_tokens)
            top_n = 40
            common_words = freq_dist.most_common(top_n)
            df_freq = pd.DataFrame(common_words, columns=['T√©rmino', 'Frecuencia'])
            
            available_words = set(df_freq['T√©rmino'])
            if st.session_state['selected_word'] is None or st.session_state['selected_word'] not in available_words:
                if not df_freq.empty: st.session_state['selected_word'] = df_freq.iloc[0]['T√©rmino']

            if len(df_freq) > 5:
                vectorizer = TfidfVectorizer(vocabulary=df_freq['T√©rmino'].values)
                X = vectorizer.fit_transform(df['str_processed'])
                kmeans = KMeans(n_clusters=min(5, len(df_freq)), random_state=42, n_init=10)
                word_to_cluster = dict(zip(vectorizer.get_feature_names_out(), kmeans.fit_predict(X.T)))
                df_freq['Grupo'] = df_freq['T√©rmino'].map(word_to_cluster).astype(str)
            else:
                df_freq['Grupo'] = '0'
                word_to_cluster = {w: '0' for w in df_freq['T√©rmino']}

            palette = px.colors.qualitative.Bold 
            unique_groups = sorted(df_freq['Grupo'].unique())
            group_color_map = {grp: palette[i % len(palette)] for i, grp in enumerate(unique_groups)}
            word_color_map = {row['T√©rmino']: group_color_map[row['Grupo']] for _, row in df_freq.iterrows()}
            def color_func(word, **kwargs): return word_color_map.get(word, '#888888')

            # --- ESTRUCTURA DE PESTA√ëAS (ACTUALIZADA) ---
            tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
                "Frecuencia & KWIC", 
                "Mapa calor", 
                "Similitud vocabularios", 
                "Red co-ocurrencias", 
                "An. correspondencias", 
                "An. sentimientos"
            ])

            # 1. FRECUENCIA & KWIC
            with tab1:
                c1, c2 = st.columns([1.2, 0.8])
                with c1:
                    st.subheader("Glosario de t√©rminos m√°s utilizados")
                    st.markdown("**Haz clic en una barra para mostrar el contexto:**")
                    fig_bar = px.bar(df_freq, x='Frecuencia', y='T√©rmino', orientation='h', color='Grupo', text='Frecuencia', color_discrete_map=group_color_map)
                    fig_bar.update_layout(yaxis=dict(categoryorder='total ascending', tickfont=dict(size=16, color='black', family="Arial Black")), xaxis=dict(showticklabels=False), showlegend=False, height=650, margin=dict(l=0,r=0,t=0,b=0))
                    fig_bar.update_traces(textposition='outside', textfont_size=18, cliponaxis=False, width=0.7)
                    event = st.plotly_chart(fig_bar, use_container_width=True, on_select="rerun", key="bar")
                    if event and event['selection']['points']: st.session_state['selected_word'] = event['selection']['points'][0]['y']
                with c2:
                    st.subheader("Nube Sem√°ntica")
                    wc = WordCloud(width=500, height=500, background_color='white', max_words=top_n, color_func=color_func, prefer_horizontal=1.0, relative_scaling=0, margin=0, min_font_size=8).generate_from_frequencies(dict(common_words))
                    fig_wc, ax = plt.subplots(figsize=(6,6))
                    ax.imshow(wc, interpolation='bilinear'); ax.axis('off')
                    plt.subplots_adjust(top=1, bottom=0, right=1, left=0, hspace=0, wspace=0)
                    st.pyplot(fig_wc)
                
                st.markdown("---"); st.markdown("### üìù An√°lisis de Contexto (KWIC)")
                c_s, c_i = st.columns([1,3])
                with c_s: m_search = st.text_input("üîç Buscar palabra:", value="")
                cur_w = m_search if m_search else st.session_state['selected_word']
                st.markdown(f"<div style='background-color:#f0f2f6; padding:15px; border-radius:10px; border-left: 6px solid #e74c3c;'><h4 style='margin:0; color:#2c3e50;'>Contextos para: <span style='color:#c0392b; font-size:1.3em;'>{cur_w}</span></h4></div>", unsafe_allow_html=True)
                if cur_w:
                    mask = df[text_col].str.contains(cur_w, case=False, na=False)
                    if len(df[mask]) > 0: st.dataframe(df[mask][[cat_cols[0], text_col]], use_container_width=True, hide_index=True)
                    else: st.warning(f"No se encontraron coincidencias directas.")

            # 2. MAPA CALOR
            with tab2:
                cat_heat = st.selectbox("Seleccione la Variable Categ√≥rica (Filas):", cat_cols)
                st.markdown(f"### {cat_heat} vs Vocabulario")
                with st.info("Gu√≠a: Residuos positivos (rojo) = sobre-uso; negativos = infra-uso. *** = Alta significancia."): pass
                
                df_exp = df.explode('tokens')
                df_heat = df_exp[df_exp['tokens'].isin(df_freq['T√©rmino'].head(20))]
                if not df_heat.empty:
                    obs = pd.crosstab(df_heat[cat_heat], df_heat['tokens'])
                    custom_colors = [[0.0, "#FFFFCC"], [0.2, "#FED976"], [0.4, "#FD8D3C"], [0.6, "#E31A1C"], [0.8, "#800026"], [1.0, "#4A0012"]]
                    st.subheader("1. Representaci√≥n Visual")
                    fig_h = px.imshow(obs, text_auto=False, aspect="auto", color_continuous_scale=custom_colors, labels=dict(x="", y=cat_heat, color="Freq"))
                    fig_h.update_layout(height=500, yaxis_title=cat_heat)
                    fig_h.update_xaxes(side="top", tickfont=dict(size=14)); fig_h.update_yaxes(tickfont=dict(size=14), tickmode='linear', dtick=1)
                    st.plotly_chart(fig_h, use_container_width=True)
                    
                    st.subheader("2. Tabla de Estad√≠sticos y Significaci√≥n")
                    chi2, p, dof, ex = chi2_contingency(obs)
                    res = (obs - ex) / np.sqrt(ex)
                    p_val = np.array(2 * (1 - norm.cdf(abs(res))))
                    stats = []
                    for c in obs.index:
                        for w in obs.columns:
                            stats.append({"Categor√≠a": c, "T√©rmino": w, "Frecuencia": int(obs.loc[c,w]), "Valor-p": f"{p_val[obs.index.get_loc(c), obs.columns.get_loc(w)]:.3f}", "Sig.": get_significance_stars(p_val[obs.index.get_loc(c), obs.columns.get_loc(w)])})
                    st.dataframe(pd.DataFrame(stats), use_container_width=False, height=400, width=800, hide_index=True)
                else: st.warning("Datos insuficientes.")

            # 3. SIMILITUD VOCABULARIOS
            with tab3:
                cat_v = st.selectbox("Comparar grupos de la variable:", cat_cols, key='voc')
                st.subheader("1. Matriz de Similitud (Jaccard)")
                grp = df.groupby(cat_v)['tokens'].apply(list)
                g_vocab = {k: set([i for s in v for i in s]) for k, v in grp.items()}
                g_list = sorted(list(g_vocab.keys())); sz = len(g_list)
                jac = np.zeros((sz, sz))
                for i in range(sz):
                    for j in range(sz):
                        u = len(g_vocab[g_list[i]].union(g_vocab[g_list[j]]))
                        jac[i,j] = len(g_vocab[g_list[i]].intersection(g_vocab[g_list[j]])) / u if u > 0 else 0
                fig_j = px.imshow(jac, x=g_list, y=g_list, text_auto='.2f', color_continuous_scale='Blues', range_color=[0,1], title=f"Jaccard: {cat_v}")
                fig_j.update_layout(height=500, xaxis=dict(tickmode='linear', dtick=1, side="top"), yaxis=dict(tickmode='linear', dtick=1))
                st.plotly_chart(fig_j, use_container_width=True)
                
                st.markdown("---"); st.subheader("2. M√©tricas de Diversidad L√©xica")
                with st.info("MTLD: Medida robusta de riqueza l√©xica (indep. longitud). Valores altos = Mayor riqueza."): pass
                div_d = []
                for k, v in grp.items():
                    fl = [i for s in v for i in s]; n=len(fl); u=len(set(fl))
                    div_d.append({"Categor√≠a": k, "Total (N)": n, "√önicas (V)": u, "TTR": round(u/n if n else 0,3), "MTLD": round(calculate_mtld(fl),2)})
                c_d1, c_d2 = st.columns([1,1])
                with c_d1: st.dataframe(pd.DataFrame(div_d), use_container_width=True, hide_index=True)
                with c_d2: st.plotly_chart(px.bar(div_d, x='Categor√≠a', y=['MTLD', 'TTR'], barmode='group'), use_container_width=True)

            # 4. RED CO-OCURRENCIAS
            with tab4:
                st.subheader("Red de co-ocurrencias")
                st.markdown("**Interpretaci√≥n:** Nodos = Palabras (Tama√±o proporcional a frecuencia). L√≠neas = Co-ocurrencia.")
                vec = CountVectorizer(max_features=40, stop_words=stopwords.words(LANG_MAP.get(lang_opt, 'spanish')))
                try:
                    Xn = vec.fit_transform(df['str_processed'])
                    adj = (Xn.T * Xn); adj.setdiag(0)
                    G = nx.from_pandas_adjacency(pd.DataFrame(adj.toarray(), index=vec.get_feature_names_out(), columns=vec.get_feature_names_out()))
                    edges_del = [(u,v) for u,v,d in G.edges(data=True) if d['weight'] < 2]
                    G.remove_edges_from(edges_del); G.remove_nodes_from(list(nx.isolates(G)))
                    
                    # Normalizaci√≥n Min-Max para tama√±os
                    node_freqs_values = [freq_dist.get(node, 1) for node in G.nodes()]
                    if node_freqs_values:
                        min_f, max_f = min(node_freqs_values), max(node_freqs_values)
                        node_sizes = [300 + ((f - min_f) / (max_f - min_f) * 2200) if max_f > min_f else 1000 for f in node_freqs_values]
                    else: node_sizes = []

                    fig_net, ax_net = plt.subplots(figsize=(7, 5))
                    pos = nx.spring_layout(G, k=0.6, seed=42)
                    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='#aaddff', alpha=0.9, ax=ax_net)
                    nx.draw_networkx_edges(G, pos, edge_color='#cccccc', width=1, ax=ax_net)
                    nx.draw_networkx_labels(G, pos, font_size=9, ax=ax_net)
                    ax_net.axis('off'); st.pyplot(fig_net)
                except Exception as e: st.warning(f"Datos insuficientes: {e}")

            # 5. AN. CORRESPONDENCIAS (NUEVO)
            with tab5:
                st.subheader("An√°lisis de Correspondencias Simple (ACS)")
                st.info("Mapa perceptual: La cercan√≠a entre puntos (Categor√≠as rojos y Palabras azules) indica asociaci√≥n.")
                
                cat_ca = st.selectbox("Seleccione Variable para ACS:", cat_cols, key='ca_cat')
                
                # Preparamos datos: Categor√≠a vs Top 30 palabras (para no saturar)
                df_exp = df.explode('tokens')
                top_30_words = df_freq['T√©rmino'].head(30).tolist()
                df_ca = df_exp[df_exp['tokens'].isin(top_30_words)]
                
                if not df_ca.empty:
                    # Tabla de contingencia
                    cont_table = pd.crosstab(df_ca[cat_ca], df_ca['tokens'])
                    
                    if cont_table.shape[0] > 1 and cont_table.shape[1] > 1:
                        # C√°lculo Matem√°tico (SVD)
                        row_coords, col_coords, singular_values = simple_correspondence_analysis(cont_table)
                        
                        # Inercia explicada (aprox)
                        inertia = singular_values**2
                        total_inertia = np.sum(inertia)
                        explained_variance = inertia / total_inertia
                        dim1_expl = explained_variance[0] * 100
                        dim2_expl = explained_variance[1] * 100
                        
                        # Graficar Biplot
                        fig_ca = go.Figure()
                        
                        # 1. Puntos de Filas (Categor√≠as) - Rojos
                        fig_ca.add_trace(go.Scatter(
                            x=row_coords[:, 0], y=row_coords[:, 1],
                            mode='markers+text',
                            text=cont_table.index,
                            textposition="top center",
                            marker=dict(size=12, color='red', symbol='square'),
                            name="Categor√≠as"
                        ))
                        
                        # 2. Puntos de Columnas (Palabras) - Azules
                        fig_ca.add_trace(go.Scatter(
                            x=col_coords[:, 0], y=col_coords[:, 1],
                            mode='markers+text',
                            text=cont_table.columns,
                            textposition="bottom center",
                            marker=dict(size=8, color='blue', opacity=0.7),
                            name="Palabras"
                        ))
                        
                        fig_ca.update_layout(
                            title=f"Mapa Perceptual (Dim 1: {dim1_expl:.1f}% + Dim 2: {dim2_expl:.1f}%)",
                            xaxis_title=f"Dimensi√≥n 1 ({dim1_expl:.1f}%)",
                            yaxis_title=f"Dimensi√≥n 2 ({dim2_expl:.1f}%)",
                            height=600,
                            showlegend=True,
                            template="plotly_white"
                        )
                        
                        # L√≠neas de ejes 0,0
                        fig_ca.add_vline(x=0, line_width=1, line_dash="dash", line_color="gray")
                        fig_ca.add_hline(y=0, line_width=1, line_dash="dash", line_color="gray")
                        
                        st.plotly_chart(fig_ca, use_container_width=True)
                    else:
                        st.warning("La tabla de contingencia es demasiado peque√±a para el an√°lisis.")
                else:
                    st.warning("Datos insuficientes con los filtros actuales.")

            # 6. AN. SENTIMIENTOS
            with tab6:
                c1, c2 = st.columns(2)
                with c1: st.plotly_chart(px.histogram(df, x='polaridad', nbins=20, title="Distribuci√≥n Polaridad", color_discrete_sequence=['teal']), use_container_width=True)
                with c2: 
                    cat_s = st.selectbox("Variable:", cat_cols, key='s')
                    st.plotly_chart(px.box(df, x=cat_s, y='polaridad', color=cat_s, title="Polaridad por Categor√≠a"), use_container_width=True)

    except Exception as e: st.error(f"Error procesando el archivo: {e}")
